import numpy as np
from .utils import overrides
import theano.tensor as T
import theano
from scipy.sparse import rand
from scipy import stats
import pickle

class Distribution(object):
    """
    interface for distributions
    """

    def __init__(self, ndims=2, nbatch=100, mjhmc=True):
        self.ndims = ndims
        self.nbatch = nbatch
        self.mjhmc = mjhmc
        self.init_X()
        self.E_count = 0
        self.dEdX_count = 0

    def E(self, X):
        self.E_count += X.shape[1]
        return self.E_val(X)

    def E_val(self, X):
        raise NotImplementedError()

    def dEdX(self, X):
        self.dEdX_count += X.shape[1]
        return self.dEdX_val(X)

    def dEdX_val(self, X):
        raise NotImplementedError()

    def init_X(self):
        """
        Sets self.Xinit to a good initial value
        """
        if self.mjhmc:
            self.cached_init_X()
        else:
            self.gen_init_X()

    def cached_init_X(self):
        """ Sets self.Xinit to cached (serialized) initial states for continuous-time samplers, generated by burn in
        *For use with continuous-time samplers only*

        :returns: None
        :rtype: none
        """
        # implement me!
        # no need to use inheritance, just store the initial states using the distribution name
        # remove this when implemented
        #Totally hardcoding this now, going to make a relative encoding afterwarsds
        print('Loading samples from cached file')
        df = pickle.load(open('poe_ndims_36_nbasis_36_nsamples_10000.pkl','r'))
        self.Xinit = df.as_matrix()[-1]




    def gen_init_X(self):
        """ Sets self.Xinit to generated initial states for the sampling particles
        *For use with discrete-time samplers only*

        :returns: None
        :rtype: None
        """
        raise NotImplementedError()

    def reset(self):
        """
        resets the object. returns self for convenience
        """
        self.E_count = 0
        self.dEdX_count = 0
        self.init_X()
        return self

    def __call__(self, X):
        """
        Convenience method for NUTS compatibility
        returns -E, -dEdX
        """
        rshp_X = X.reshape(len(X), 1)
        E = float(self.E(rshp_X))
        dEdX = self.dEdX(rshp_X).T[0]
        return -E, -dEdX

class Gaussian(Distribution):
    def __init__(self, ndims=2, nbatch=100, log_conditioning=6):
        """
        Energy function, gradient, and hyperparameters for the "ill
        conditioned Gaussian" example from the LAHMC paper.
        """
        self.conditioning = 10**np.linspace(-log_conditioning, 0, ndims)
        self.J = np.diag(self.conditioning)
        self.description = '%dD Anisotropic Gaussian, %g self.conditioning'%(ndims, 10**log_conditioning)
        super(Gaussian, self).__init__(ndims, nbatch)

    @overrides(Distribution)
    def E_val(self, X):
        return np.sum(X*np.dot(self.J,X), axis=0).reshape((1,-1))/2.

    @overrides(Distribution)
    def dEdX_val(self, X):
        return np.dot(self.J,X)/2. + np.dot(self.J.T,X)/2.

    @overrides(Distribution)
    def gen_init_X(self):
        self.Xinit = (1./np.sqrt(self.conditioning).reshape((-1,1))) * np.random.randn(self.ndims,self.nbatch)

class RoughWell(Distribution):
    def __init__(self, ndims=2, nbatch=100, scale1=100, scale2=4):
        """
        Energy function, gradient, and hyperparameters for the "rough well"
        example from the LAHMC paper.
        """
        self.scale1 = scale1
        self.scale2 = scale2
        self.description = '{} Rough Well'.format(ndims)
        super(RoughWell, self).__init__(ndims, nbatch)

    @overrides(Distribution)
    def E_val(self, X):
        cosX = np.cos(X*2*np.pi/self.scale2)
        E = np.sum((X**2) / (2*self.scale1**2) + cosX, axis=0).reshape((1,-1))
        return E

    @overrides(Distribution)
    def dEdX_val(self, X):
        sinX = np.sin(X*2*np.pi/self.scale2)
        dEdX = X/self.scale1**2 + -sinX*2*np.pi/self.scale2
        return dEdX

    @overrides(Distribution)
    def gen_init_X(self):
        self.Xinit = self.scale1 * np.random.randn(self.ndims, self.nbatch)

class MultimodalGaussian(Distribution):
    def __init__(self, ndims=2, nbatch=100, separation=3):
        self.sep_vec = np.array([separation] * nbatch +
                                [0] * (ndims - 1) * nbatch).reshape(ndims, nbatch)
        # separated along first axis
        self.sep_vec[0] += separation
        super(MultimodalGaussian, self).__init__(ndims, nbatch)

    @overrides(Distribution)
    def E_val(self, X):
        trim_sep_vec = self.sep_vec[:, :X.shape[1]]
        return -np.log(np.exp(-np.sum((X + trim_sep_vec)**2, axis=0)) +
                       np.exp(-np.sum((X - trim_sep_vec)**2, axis=0)))

    @overrides(Distribution)
    def dEdX_val(self, X):
        # allows for partial batch size
        trim_sep_vec = self.sep_vec[:, :X.shape[1]]
        common_exp = np.exp(np.sum(4 * trim_sep_vec * X, axis=0))
        # floating point hax
        return ((2 * ((X - trim_sep_vec) * common_exp + trim_sep_vec + X)) /
                (common_exp + 1))


    @overrides(Distribution)
    def init_X(self):
        # okay, this is pointless... sep vecs cancel
        self.Xinit = ((np.random.randn(self.ndims, self.nbatch) + self.sep_vec) +
                (np.random.randn(self.ndims, self.nbatch) - self.sep_vec))

class TestGaussian(Distribution):

    def __init__(self, ndims=2, nbatch=100, sigma=1.):
        """Simple default unit variance gaussian for testing samplers
        """
        super(TestGaussian, self).__init__(ndims, nbatch)
        self.sigma = sigma

    @overrides(Distribution)
    def E_val(self, X):
        return np.sum(X**2, axis=0).reshape((1,-1))/2./self.sigma**2

    @overrides(Distribution)
    def dEdX_val(self, X):
        return X/self.sigma**2

    @overrides(Distribution)
    def gen_init_X(self):
        self.Xinit = np.random.randn(self.ndims, self.nbatch)

class ProductOfT(Distribution):

    def __init__(self,ndims=36,nbasis=36,nbatch=100,lognu=None,W=None,b=None):
        """ Product of T experts, assumes a fixed W that is sparse and alpha that is
        """
        if ndims != nbasis:
            raise NotImplementedError("Initializer only works for ndims == nbasis")
        self.ndims=ndims
        self.nbasis=nbasis
        self.nbatch=nbatch
        if W is  None:
            # rand_val = rand(ndims,nbasis,density=0.125)
            # W = rand_val + rand_val.T
            W = np.eye(ndims, nbasis)
        self.W = theano.shared(np.array(W,dtype='float32'),'W')
        if lognu is None:
            pre_nu = np.random.rand(nbasis,)*2 + 2.1
        else:
            pre_nu = np.exp(lognu)
        self.nu = theano.shared(np.array(pre_nu,dtype='float32'),'nu')
        if b is None:
            b = np.zeros((nbasis,))
        self.b = theano.shared(np.array(b,dtype='float32'),'b')
        X = T.matrix()
        E = self.E_def(X)
        dEdX = T.grad(T.sum(E),X)
        #@overrides(Distribution)
        self.E_val=theano.function([X],E,allow_input_downcast=True)
        #@overrides(Distribution)
        self.dEdX_val = theano.function([X],dEdX,allow_input_downcast=True)
        super(ProductOfT,self).__init__(ndims,nbatch)

    def E_def(self,X):
        """
        energy for a POE with student's-t expert in terms of:
                samples [# samples]x[# dimensions] X
                receptive fields [# dimensions]x[# experts] W
                biases [# experts] b
                degrees of freedom [# experts] nu
        """
        rshp_b = self.b.reshape((1,-1))
        rshp_nu = self.nu.reshape((1, -1))
        alpha = (rshp_nu + 1.)/2.
        E_perexpert = alpha*T.log(1 + ((T.dot(X.T,self.W) + rshp_b)/rshp_nu)**2)
        E = T.sum(E_perexpert, axis=1).reshape((1,-1))
        return E


    '''
    #Turning this off for now. We should figure out what we want to do with this moving fwd
    @overrides(Distribution)
    def gen_init_X(self):
		Zinit = np.zeros((self.ndims, self.nbatch))
		for ii in xrange(self.ndims):
			Zinit[ii] = stats.t.rvs(self.nu.get_value()[ii], size=self.nbatch)

		Yinit = Zinit - self.b.get_value().reshape((-1, 1))
		self.Xinit = np.dot(np.linalg.inv(self.W.get_value()), Yinit)
    '''
